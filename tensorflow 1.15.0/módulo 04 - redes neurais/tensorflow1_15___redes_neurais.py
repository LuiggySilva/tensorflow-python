# -*- coding: utf-8 -*-
"""tensorflow1_15___redes_neurais.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pst-3uIOhUlgOD_n0xRjy-Z8bwQoD3Wn
"""

import tensorflow as tf
import numpy as np

"""> # **Aula 016** - Perceptron de uma camada - **Tensorflow: Redes Neurais - Classificação e Regressão**"""

def step_function(x):
  # x >= 1 return 1.0 else return 0.0
  return tf.cast( tf.to_float(tf.math.greater_equal(x, 1)) , tf.float64)

# - exemplo: porta lógica OR -

'''
 | A | B | A or B |
 | 0 | 0 |   0    | 
 | 0 | 1 |   1    |  
 | 1 | 0 |   1    |  
 | 1 | 1 |   1    |
'''

# entrada
X = np.array([[0.0, 0.0],
              [0.0, 1.0],
              [1.0, 0.0],
              [1.0, 1.0]])

# saida
y = np.array([[0.0],
              [1.0],
              [1.0],
              [1.0]])

# pesos
# [2, 1] = tamanho da matriz de zeros 2 linhas x 1 coluna - 2 linhas por conta das duas entradas
W = tf.Variable(tf.zeros([2, 1], dtype = tf.float64))

camada_saida = tf.matmul(X, W)
camada_saida_ativacao = step_function(camada_saida)

# calculo e ajuste dos erros - peso(n+1) = peso(n) + (taxa_aprendizagem * entrada * erro) -
erro = tf.subtract(y, camada_saida_ativacao)

# transpose_a = True -> realizando o calculo com a matriz transposta de X 
# para não ter erro com as dimenções das matrizes no cálculo
delta = tf.matmul(X, erro, transpose_a=True)

# tf.assigin(W, ...) -> permite que a váriavel W seja atualizada durante a execução
treinamento = tf.assign(W, tf.add(W, tf.multiply(delta, 0.1)) )

init = tf.global_variables_initializer()

with tf.Session() as sess:
  sess.run(init)

  print(f'X\n{X}', end='\n'*2)
  print(f'y\n{y}', end='\n'*2)  

  print(f'W\n{sess.run(W)}', end='\n'*2)
  print(f'camada_saida\n{sess.run(camada_saida)}', end='\n'*2)
  print(f'camada_saida_ativacao\n{sess.run(camada_saida_ativacao)}', end='\n'*2)
  
  print(f'erro\n{sess.run(erro)}', end='\n'*2)
  print(f'treinamento\n{sess.run(treinamento)}', end='\n'*2)

  epochs = 0
  for i in range(15):
    epochs += 1
    # o erro_total é um vetor com os erros de cada registro
    erro_total, _ = sess.run([erro, treinamento])
    erro_soma = tf.reduce_sum(erro_total)

    print(f'época {epochs} - erro_total: {sess.run(erro_soma)}')

    if(erro_soma.eval() == 0.0):
      break
  
  W_final = sess.run(W)
  print( '\n' + f'W_final\n{W_final}')

# testando a rede neural

camada_saida_teste = tf.matmul(X, W_final)
camada_saida_ativacao_teste = step_function(camada_saida_teste)

print(' - teste do exemplo da porta lógica OR - \n')
with tf.Session() as sess:
  sess.run(init)
  print(f'teste com entrada\n{X}' + '\n'*2 + f'pesos\n{W_final}' + '\n'*2 + f'resultado\n{sess.run(camada_saida_ativacao_teste)}')

"""> # **Aula 017** - Classificação binária XOR - **Tensorflow: Redes Neurais - Classificação e Regressão**"""

# - exemplo: porta lógica XOR - problema não linearmente separavel

'''
 | A | B | A xor B |
 | 0 | 0 |    0    | 
 | 0 | 1 |    1    |  
 | 1 | 0 |    1    |  
 | 1 | 1 |    0    |
'''

'''
Estrutura da rede neural implementada abaixo (sem o BIAS)

E = Entrada | O = Camada oculta | S = Saida

  E    O    S
       ◯ ↘
  ◯ ⇶            
       ◯ →  □□
  ◯ ⇶           
       ◯ ↗

'''

# entrada
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

# saida
y = np.array([[0],
              [1],
              [1],
              [0]])

neuronios_entrada = 2
neuronios_camada_oculta = 3
neuronios_saida = 1

# pesos
W = {
  'oculta': tf.Variable(tf.random_normal([neuronios_entrada, neuronios_camada_oculta]), name='w_camada_oculta'),
  'saida': tf.Variable(tf.random_normal([neuronios_camada_oculta, neuronios_saida]), name='w_camada_saida')
}

bias = {
    'oculta': tf.Variable(tf.random_normal([neuronios_camada_oculta]), name='w_bias_oculta'),
    'saida': tf.Variable(tf.random_normal([neuronios_saida]), name='w_bias_saida')
}

init = tf.global_variables_initializer()
with tf.Session() as sess:
  sess.run(init)
  print(f'w_camada_oculta\n{sess.run(W["oculta"])}', end='\n'*2)
  print(f'w_camada_saida\n{sess.run(W["saida"])}', end='\n'*2)

  print(f'w_bias_camada_oculta\n{sess.run(bias["oculta"])}', end='\n'*2)
  print(f'w_bias_camada_saida\n{sess.run(bias["saida"])}', end='\n'*2)

xph = tf.placeholder(tf.float32, [len(X), neuronios_entrada], name='xph')
yph = tf.placeholder(tf.float32, [len(y), neuronios_saida], name='yph')

camada_oculta = tf.add(bias['oculta'], tf.matmul(xph, W['oculta']))
camada_oculta_ativacao = tf.sigmoid(camada_oculta)

camada_saida = tf.add(bias['saida'], tf.matmul(camada_oculta_ativacao, W['saida']))
camada_saida_ativacao = tf.sigmoid(camada_saida)

erro = tf.losses.mean_squared_error(yph, camada_saida_ativacao)
otimizador = tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(erro)

with tf.Session() as sess:
  sess.run(init)
  print(f'camada_oculta\n{sess.run(camada_oculta_ativacao, feed_dict={xph:X})}', end='\n'*2)
  print(f'camada_saida\n{sess.run(camada_saida_ativacao, feed_dict={xph:X})}', end='\n'*2)

  for epochs in range(10000):
    erro_medio = 0
    _, custo = sess.run([otimizador, erro], feed_dict={xph:X, yph:y})
    
    if(epochs % 200 == 0):
      erro_medio += custo / 4
      print(f'erro_medio - {erro_medio}')
  
  W_final, bias_final = sess.run([W, bias])

W_final

bias_final

# testando a rede neural
camada_oculta_teste = tf.add(bias_final['oculta'], tf.matmul(xph, W_final['oculta']))
camada_oculta_ativacao_teste = tf.sigmoid(camada_oculta_teste)

camada_saida_teste = tf.add(bias_final['saida'], tf.matmul(camada_oculta_ativacao_teste, W_final['saida']))
camada_saida_ativacao_teste = tf.sigmoid(camada_saida_teste)

print(' - teste do exemplo da porta lógica XOR - \n')
with tf.Session() as sess:
  sess.run(init)
  print(f'teste com entrada\n{X}' + '\n'*2 + f'resultado\n{sess.run(camada_saida_ativacao_teste, feed_dict={xph:X})}')

"""> # **Aula 018** - Classificação multiclasse - **Tensorflow: Redes Neurais - Classificação e Regressão**"""

from sklearn import datasets
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# dataset iris info - https://archive.ics.uci.edu/ml/datasets/Iris
iris = datasets.load_iris()

iris.data

iris.target

iris.target_names

'''
Estrutura da rede neural implementada abaixo (sem o BIAS)

E = Entrada | O = Camada oculta | S = Saida

  E   O    S
  ◯ ⇶ ◯ ⇶
           ◯  
  ◯ ⇶ ◯ ⇶        
           ◯
  ◯ ⇶ ◯ ⇶          
           ◯
  ◯ ⇶ ◯ ⇶

Três neuronios de saida onde cada neuroino é uma classe e sua 
saida representa a probabilidade da entrada ser dessa classe
'''

X = iris.data
y = iris.target

# pre-processamento dos dados
scaler_x = StandardScaler()
X = scaler_x.fit_transform(X)
y = y.reshape(-1, 1)

# Divide cada atributo na quantidade existente de classes para que a saida da probabilidade de cada uma seja possivel
# o numero é o indice do numero 1 na lista
onehot = OneHotEncoder()
y = onehot.fit_transform(y).toarray()
y

# separando dados para treinamento e testes
x_training, x_test, y_training, y_test = train_test_split(X, y, test_size=0.3)

# definindo estrutura da rede neural
neuronios_entrada = X.shape[1]
neuronios_camada_oculta = int(np.ceil((X.shape[1] + y.shape[1]) / 2))
neuronios_saida = y.shape[1]

# pesos
W = {
  'oculta': tf.Variable(tf.random_normal([neuronios_entrada, neuronios_camada_oculta])),
  'saida': tf.Variable(tf.random_normal([neuronios_camada_oculta, neuronios_saida]))
}

bias = {
  'oculta': tf.Variable(tf.random_normal([neuronios_camada_oculta])),
  'saida': tf.Variable(tf.random_normal([neuronios_saida]))
}

# placeholders
xph = tf.placeholder('float', [None, neuronios_entrada])
yph = tf.placeholder('float', [None, neuronios_saida])

def mlp(x, W, bias):
  camada_oculta = tf.add(tf.matmul(x, W['oculta']), bias['oculta'])
  camada_oculta_ativacao = tf.nn.relu(camada_oculta)

  camada_saida = tf.add( tf.matmul(camada_oculta_ativacao, W['saida']), bias['saida'])
  return camada_saida

modelo = mlp(xph, W, bias)
erro = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits= modelo, labels= yph) )
otimizador = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(erro)
batch_size = 8

init = tf.global_variables_initializer()
with tf.Session() as sess:
  sess.run(init)
  for epoch in range(3000):
    erro_medio = 0.0
    batch_total = len(x_training) // batch_size
    X_batches = np.array_split(x_training, batch_total)
    Y_batches = np.array_split(y_training, batch_total)
    for i in range(batch_total):
      x_batch, y_batch = X_batches[i], Y_batches[i]
      _, custo = sess.run([otimizador, erro], feed_dict={xph: x_batch, yph: y_batch})
      erro_medio += custo / batch_total
    if(epoch % 500 == 0):
      print(f'época {epoch} - erro_medio - {erro_medio}')
  
  W_final, bias_final = sess.run([W, bias])

W_final

bias_final

# previsoes
previsoes_teste = mlp(xph, W_final, bias_final)

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  # previsao pura
  r1 = sess.run(previsoes_teste, feed_dict={xph: x_test})
  # probabilidade
  r2 = sess.run(tf.nn.softmax(r1))
  # dado limpo
  r3 = sess.run(tf.arg_max(r2, 1))

# comparando previsoes com a saida correta
y_test2 = np.argmax(y_test, 1)

taxa_acerto = accuracy_score(y_test2, r3)
taxa_acerto

"""> # **Aula 018** - Base de dados de dígitos manuscritos - **Tensorflow: Redes Neurais - Classificação e Regressão**"""

from tensorflow.examples.tutorials.mnist import input_data
import matplotlib.pyplot as plt

mnist = input_data.read_data_sets('mnist/', one_hot=True)

len(mnist.train.images) + len(mnist.test.images)

x_training = mnist.train.images
print(f'x_training.shape - {x_training.shape}')
print(f'x_training[0]\n{x_training[0]}', end='\n'*2)
y_training = mnist.train.labels
print(f'y_training.shape - {y_training.shape}')
print(f'y_training[0] - {y_training[0]}', end='\n'*2)
x_test = mnist.test.images
print(f'x_test.shape - {x_test.shape}')
print(f'x_test[0]\n{x_test[0]}', end='\n'*2)
y_test = mnist.test.labels
print(f'y_test.shape - {y_test.shape}')
print(f'y_test[0] - {y_test[0]}')

img_index = 6140
plt.imshow(x_training[img_index].reshape(28,28), cmap='gray')
plt.title(f'Classe: {np.argmax(y_training[img_index])}', fontdict={'size':25})
print('Visualizando dado do dataset', end='\n'*2)

neuronios_entrada = x_training.shape[1]
# fórmula para quantidade de neuronios para camada oculta: x = numero_entradas + quantidade_classes // 2
camada_oculta_1 = (x_training.shape[1] + y_training.shape[1]) // 2
camada_oculta_2 = camada_oculta_1
camada_oculta_3 = camada_oculta_1
neuronios_saida =  y_training.shape[1]

neuronios_entrada

camada_oculta_1

neuronios_saida

'''
Estrutura da rede neural implementada (sem o BIAS)

E = Entrada | O = Camada oculta | S = Saida

  E   O   O   O    S
  ◯ ⇶ ◯ ⇶ ◯ ⇶ ◯ ⇶
                   ◯  
  ◯ ⇶ ◯ ⇶ ◯ ⇶ ◯ ⇶        
                   ◯
  ◯ ⇶ ◯ ⇶ ◯ ⇶ ◯ ⇶          
                   ◯
  ◯ ⇶ ◯ ⇶ ◯ ⇶ ◯ ⇶  .
  .   .   .   .    .
  .   .   .   .    .
  .   .   .   .    .

Entrada - 784 neuronios
Camada oculta - 397 neuronios (em cada uma das trẽs camadas ocultas)
Saida - 10 neuronios
'''

# pesos
W = {
  'oculta1': tf.Variable(tf.random_normal([neuronios_entrada, camada_oculta_1])), 
  'oculta2': tf.Variable(tf.random_normal([camada_oculta_1,   camada_oculta_2])),
  'oculta3': tf.Variable(tf.random_normal([camada_oculta_2,   camada_oculta_3])),
  'saida': tf.Variable(tf.random_normal([camada_oculta_3, neuronios_saida]))
}

bias = {
  'oculta1': tf.Variable(tf.random_normal([camada_oculta_1])),
  'oculta2': tf.Variable(tf.random_normal([camada_oculta_2])),
  'oculta3': tf.Variable(tf.random_normal([camada_oculta_3])),
  'saida': tf.Variable(tf.random_normal([neuronios_saida]))
}

def mlp(x, W, bias):
  # camada_oculta_N = função_ativação ( bias (X,  multiplicando_entrada_pesos( Y, Z ) ) )
  camada_oculta_1 = tf.nn.relu( tf.add( bias['oculta1'], tf.matmul(x, W['oculta1'] ) ) )
  camada_oculta_2 = tf.nn.relu( tf.add( bias['oculta2'], tf.matmul(camada_oculta_1, W['oculta2'] ) ) )
  camada_oculta_3 = tf.nn.relu( tf.add( bias['oculta3'], tf.matmul(camada_oculta_2, W['oculta3'] ) ) )
  camada_saida = tf.add(bias['saida'], tf.matmul(camada_oculta_3, W['saida']))
  return camada_saida

xph = tf.placeholder('float', [None, neuronios_entrada])
yph = tf.placeholder('float', [None, neuronios_saida])

modelo = mlp(xph, W, bias)
erro = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits= modelo, labels= yph))
otimizador = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(erro)

# previsoes
init = tf.global_variables_initializer()
previsoes_teste = tf.nn.softmax(modelo)
previsoes_corretas = tf.equal(tf.argmax(previsoes_teste, 1), tf.argmax(yph, 1))
taxa_acerto = tf.reduce_mean(tf.cast(previsoes_corretas, tf.float32))

with tf.Session() as sess:
  sess.run(init)
  print('Treinamento')
  for epoch in range(5000):
    X_batch, y_batch = mnist.train.next_batch(128)
    _, custo = sess.run([otimizador, erro], feed_dict={xph: X_batch, yph: y_batch})

    if(epoch % 100 == 0):
      acc = sess.run([taxa_acerto], feed_dict={xph: X_batch, yph: y_batch})
      print(f'época {epoch} -> erro: {custo} | acc: {acc}')

  print('\nTaxa de acerto')
  print(sess.run(taxa_acerto, feed_dict={xph: x_test, yph: y_test}))

"""> # **Aula 019** - Classificação com Estimators - **Tensorflow: Redes Neurais - Classificação e Regressão**"""

import pandas as pd

base = pd.read_csv('census.csv')
base.head()

base.income.unique()

def converte_classe(rotulo):
  if(rotulo == ' <=50K'):
    return 1
  else:
    return 0
base.income = base.income.apply(converte_classe)
base.income.unique()

X = base.drop('income', axis=1)
y = base.income

x_training, x_test, y_training, y_test = train_test_split(X, y, test_size=0.3)
base.columns

# colunas categoricas
workclass      = tf.feature_column.categorical_column_with_hash_bucket(key = 'workclass', hash_bucket_size=100)
education      = tf.feature_column.categorical_column_with_hash_bucket(key = 'education', hash_bucket_size=100)
marital_status = tf.feature_column.categorical_column_with_hash_bucket(key = 'marital-status', hash_bucket_size=100)
occupation     = tf.feature_column.categorical_column_with_hash_bucket(key = 'occupation', hash_bucket_size=100)
relationship   = tf.feature_column.categorical_column_with_hash_bucket(key = 'relationship', hash_bucket_size=100)
race           = tf.feature_column.categorical_column_with_hash_bucket(key = 'race', hash_bucket_size=100)
sex            = tf.feature_column.categorical_column_with_vocabulary_list(key = 'sex', vocabulary_list=[' Male', ' Female'])
native_country = tf.feature_column.categorical_column_with_hash_bucket(key = 'native-country', hash_bucket_size=100)

# colunas numéricas
age           = tf.feature_column.numeric_column(key = 'age')
final_weight  = tf.feature_column.numeric_column(key = 'final-weight')
capital_gain  = tf.feature_column.numeric_column(key = 'capital-gain')
education_num = tf.feature_column.numeric_column(key = 'education-num')
capital_loss  = tf.feature_column.numeric_column(key = 'capital-loos')
hour          = tf.feature_column.numeric_column(key = 'hour-per-week')

# colunas
colunas = [age, workclass, final_weight, education, education_num,
           marital_status, occupation, relationship, race, sex,
           capital_gain, capital_loss, hour, native_country]

embedde_workclass = tf.feature_column.embedding_column(workclass, dimension=len(base.workclass.unique()))
embedde_education = tf.feature_column.embedding_column(education, dimension=len(base.education.unique()))
embedde_marital_status = tf.feature_column.embedding_column(marital_status, dimension=len(base['marital-status'].unique()))
embedde_occupation = tf.feature_column.embedding_column(occupation, dimension=len(base.occupation.unique()))
embedde_relationship = tf.feature_column.embedding_column(relationship, dimension=len(base.relationship.unique()))
embedde_race = tf.feature_column.embedding_column(race, dimension=len(base.race.unique()))
embedde_sex = tf.feature_column.embedding_column(sex, dimension=len(base.sex.unique()))
embedde_native_country = tf.feature_column.embedding_column(native_country, dimension=len(base['native-country'].unique()))

# colunas_rna
colunas_rna = [age, embedde_workclass, final_weight, embedde_education, education_num,
               embedde_marital_status, embedde_occupation, embedde_relationship, embedde_race, embedde_sex,
               capital_gain, capital_loss, hour, embedde_native_country]

# funcoes
training_function = tf.estimator.inputs.pandas_input_fn(x= x_training, y= y_training, batch_size= 32,
                                                        num_epochs= None, shuffle= True)

# hidden_units = lista de camadas ocultas, o número é a quantidade de neuronios da camada
classificator = tf.estimator.DNNClassifier(hidden_units= [8, 8], feature_columns= colunas_rna, n_classes= 2)
classificator.train(input_fn= training_function, steps= 10000)

# testes
test_function = tf.estimator.inputs.pandas_input_fn(x= x_test, y= y_test, batch_size= 32,
                                                    num_epochs= 1, shuffle= False)
classificator.evaluate(input_fn= test_function)

"""> # **Aula 020** - Padronização com TensorFlow - **Tensorflow: Redes Neurais - Classificação e Regressão**"""

import pandas as pd

base = pd.read_csv('census.csv')
base.head()

"""# Normalização **(máximo(x) e mínimo(x) onde x é da base vai ser normalizada)**
> *x = (x - mínimo(x)) / (máximo(x) - mínimo(x))*

# Padronização **(média(x) é a média de todos valores da base que vai ser padronizada e desvio_padrão(x) é quanto o x desvia da média para cima ou para baixo)**
> *x = (x - média(x) / desvio_padrão(x))*
"""

base.age.mean() # média

base.age.std() # desvio_padrão

def padroniza_age(valor):
  mean = base.age.mean()
  std = base.age.std()
  return tf.divide(tf.subtract( tf.cast(valor, tf.float32), tf.constant(mean)), tf.constant(std))

def padroniza_final_weight(valor):
  mean = base['final-weight'].mean()
  std = base['final-weight'].std()
  return tf.divide(tf.subtract( tf.cast(valor, tf.float32), tf.constant(mean)), tf.constant(std))

def padroniza_capital_gain(valor):
  mean = base['capital-gain'].mean()
  std = base['capital-gain'].std()
  return tf.divide(tf.subtract( tf.cast(valor, tf.float32), tf.constant(mean)), tf.constant(std))

def padroniza_education_num(valor):
  mean = base['education-num'].mean()
  std = base['education-num'].std()
  return tf.divide(tf.subtract( tf.cast(valor, tf.float32), tf.constant(mean)), tf.constant(std))

def padroniza_capital_loos(valor):
  mean = base['capital-loos'].mean()
  std = base['capital-loos'].std()
  return tf.divide(tf.subtract( tf.cast(valor, tf.float32), tf.constant(mean)), tf.constant(std))

def padroniza_hour(valor):
  mean = base.hour.mean()
  std = base.hour.std()
  return tf.divide(tf.subtract( tf.cast(valor, tf.float32), tf.constant(mean)), tf.constant(std))

# colunas numéricas
age           = tf.feature_column.numeric_column(key = 'age', normalizer_fn=padroniza_age)
final_weight  = tf.feature_column.numeric_column(key = 'final-weight', normalizer_fn=padroniza_final_weight)
capital_gain  = tf.feature_column.numeric_column(key = 'capital-gain', normalizer_fn=padroniza_capital_gain)
education_num = tf.feature_column.numeric_column(key = 'education-num', normalizer_fn=padroniza_education_num)
capital_loos  = tf.feature_column.numeric_column(key = 'capital-loos', normalizer_fn=padroniza_capital_loos)
hour          = tf.feature_column.numeric_column(key = 'hour-per-week', normalizer_fn=padroniza_hour)

"""> # **Aula 021** - Regressão com Estimators - **Tensorflow: Redes Neurais - Classificação e Regressão**"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error

base = pd.read_csv('house-prices.csv')
base.head()

base.columns

colunas_usadas = ['price', 'bedrooms', 'bathrooms', 'sqft_living',
                  'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',
                  'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',
                  'lat', 'long']
base = pd.read_csv('house-prices.csv', usecols= colunas_usadas)
base.head()

# normalizando os dados
scaler_attrs = MinMaxScaler()

base[['bedrooms', 'bathrooms', 'sqft_living',
      'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',
      'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',
      'lat', 'long']] = scaler_attrs.fit_transform(base[['bedrooms', 'bathrooms', 'sqft_living',
                                                    'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',
                                                    'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',
                                                    'lat', 'long']])
base.head()

scaler_preco = MinMaxScaler()
base[['price']] = scaler_preco.fit_transform(base[['price']])
base.head()

X = base.drop('price', axis=1)
y = base.price

colunas_previsoras = colunas_usadas[1:17]
colunas = [tf.feature_column.numeric_column(key = c) for c in colunas_previsoras]

x_training, x_test, y_training, y_test = train_test_split(X, y, test_size= 0.3)

training_function = tf.estimator.inputs.pandas_input_fn(x= x_training, y= y_training, batch_size= 32,
                                                        num_epochs= None, shuffle= True)
regressor = tf.estimator.DNNRegressor(hidden_units=[8,8,8], feature_columns= colunas)
regressor.train(input_fn=training_function, steps= 20000)

predict_function = tf.estimator.inputs.pandas_input_fn(x= x_test, y= y_test, shuffle= False)
predicts = regressor.predict(input_fn= predict_function)

predict_values = []
for p in list(predicts):
  predict_values.append(p['predictions'][0])

predict_values = np.asarray(predict_values).reshape(-1,1)
predict_values = scaler_preco.inverse_transform(predict_values)

y_test2 = y_test.values.reshape(-1,1)
y_test2 = scaler_preco.inverse_transform(y_test2)

mae = mean_absolute_error(y_test2, predict_values)
mae