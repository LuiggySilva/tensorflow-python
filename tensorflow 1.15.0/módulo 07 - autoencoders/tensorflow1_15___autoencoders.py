# -*- coding: utf-8 -*-
"""tensorflow1_15___autoencoders.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y0Qxf3ef6Q5vwdLs6etRAoDMmssQ_DXN
"""

import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

"""> # **Aula 023** - Redução de dimensionalidade - **Tensorflow: Autoencoders**
> Exemplo: Prever se cliente vai pagar ou não um empréstimo
"""

base = pd.read_csv('credit-data.csv')
base = base.drop('i#clientid', axis= 1)
base = base.dropna()
base.head()

base.shape

# preprocessing
scaler_x = StandardScaler()
base[['income',	'age',	'loan']] = scaler_x.fit_transform(base[['income',	'age',	'loan']])
base.head()

X = base.drop('c#default', axis= 1)
y = base['c#default']

colunas = [tf.feature_column.numeric_column(key = c) for c in X.columns]
colunas

x_training, x_test, y_training, y_test = train_test_split(X, y, test_size= 0.3)

training_function = tf.estimator.inputs.pandas_input_fn(x= x_training, y= y_training,
                                                        batch_size= 8, num_epochs= None,
                                                        shuffle= True)

classificator = tf.estimator.DNNClassifier(feature_columns= colunas, hidden_units= [4,4])
classificator.train(input_fn= training_function, steps= 1000)

test_function = tf.estimator.inputs.pandas_input_fn(x= x_test, y= y_test,
                                                    batch_size= 8, num_epochs= 1000,
                                                    shuffle= False)

test_metrics = classificator.evaluate(input_fn= test_function, steps= 1000)

"""> test_metrics without linear autoencoder results"""

test_metrics

from tensorflow.contrib.layers import fully_connected
from sklearn.metrics import mean_absolute_error

base = pd.read_csv('credit-data.csv')
base = base.drop('i#clientid', axis= 1)
base = base.dropna()

scaler_x = StandardScaler()
base[['income',	'age',	'loan']] = scaler_x.fit_transform(base[['income',	'age',	'loan']])

X = base.drop('c#default', axis= 1)
y = base['c#default']

neuronios_entrada = 3
neuronios_oculta = 2
neuronios_saida = neuronios_entrada

xph = tf.placeholder(tf.float32, shape= [None, neuronios_entrada])

camada_oculta = fully_connected(inputs = xph, num_outputs= neuronios_oculta, activation_fn= None)
camada_saida = fully_connected(inputs= camada_oculta, num_outputs= neuronios_saida)

erro = tf.losses.mean_squared_error(labels= xph, predictions= camada_saida)
otimizador = tf.train.AdamOptimizer(learning_rate= 0.01)
treinamento = otimizador.minimize(erro)

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  for epoch in range(1000):
    custo, _ = sess.run([erro, treinamento], feed_dict={xph: X})
    if(epoch % 100 == 0):
      print(f'epoch {epoch} - erro: {custo}')
  x2d_encode = sess.run(camada_oculta, feed_dict= {xph: X})
  x3d_encode = sess.run(camada_saida, feed_dict= {xph: X})

x2d_encode

x3d_encode

x2 = scaler_x.inverse_transform(X)
x2

x3d_decoder2 = scaler_x.inverse_transform(x3d_encode)
x3d_decoder2

mae_income = mean_absolute_error(x2[:,0], x3d_decoder2[:,0])
mae_income

mae_age = mean_absolute_error(x2[:,1], x3d_decoder2[:,1])
mae_age

mae_loan = mean_absolute_error(x2[:,2], x3d_decoder2[:,2])
mae_loan

x_encode = pd.DataFrame({'atributo1':x2d_encode[:,0], 'atributo2':x2d_encode[:,1], 'classe':y})
x_encode.head()

colunas = [tf.feature_column.numeric_column(key = c) for c in x_encode.columns]
x_training, x_test, y_training, y_test = train_test_split(x_encode, y, test_size= 0.3)

training_function = tf.estimator.inputs.pandas_input_fn(x= x_training, y= y_training,
                                                        batch_size= 8, num_epochs= None,
                                                        shuffle= True)

classificator = tf.estimator.DNNClassifier(feature_columns= colunas, hidden_units= [4,4])
classificator.train(input_fn= training_function, steps= 1000)

test_function = tf.estimator.inputs.pandas_input_fn(x= x_test, y= y_test,
                                                    batch_size= 8, num_epochs= 1000,
                                                    shuffle= False)

test_metrics = classificator.evaluate(input_fn= test_function, steps= 1000)

"""> test_metrics with linear autoencoders results"""

test_metrics

"""> # Exemplo: Dataset MNIST (stacked autoencoder)"""

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('mnist/', one_hot= True)
X = mnist.train.images

plt.imshow(X[5].reshape(28,28))

# Estrutura da rede/autoenoder
# entrada 784 -> oculta1 128 -> oculta2 64 -> oculta3 128 -> saida 784

# encoder
neuronios_entrada = 784
neuronios_oculta1 = 128
# encoded data/image
neuronios_oculta2 = 64
# decoder
neuronios_oculta3 = neuronios_oculta1
neuronios_saida = neuronios_entrada

tf.reset_default_graph()

xph = tf.placeholder(tf.float32, shape= [None, neuronios_entrada])

# Usar outros inicializadores de pesos em redes neurais complexas
# Xavier: sigmoid
# He: relu
inicializador = tf.variance_scaling_initializer()

# pesos
W = {
  'encoder_oculta1': tf.Variable(inicializador([neuronios_entrada, neuronios_oculta1])),
  'encoder_oculta2': tf.Variable(inicializador([neuronios_oculta1, neuronios_oculta2])),
  'decoder_oculta3': tf.Variable(inicializador([neuronios_oculta2, neuronios_oculta3])),
  'decoder_saida':   tf.Variable(inicializador([neuronios_oculta3, neuronios_saida])),
}

bias = {
  'encoder_oculta1': tf.Variable(inicializador([neuronios_oculta1])),
  'encoder_oculta2': tf.Variable(inicializador([neuronios_oculta2])),
  'decoder_oculta3': tf.Variable(inicializador([neuronios_oculta3])),
  'decoder_saida':   tf.Variable(inicializador([neuronios_saida])),
}

camada_oculta1 = tf.nn.relu( tf.add( bias['encoder_oculta1'], tf.matmul(xph, W['encoder_oculta1']) ) )
camada_oculta2 = tf.nn.relu( tf.add( bias['encoder_oculta2'], tf.matmul(camada_oculta1, W['encoder_oculta2']) ) )
camada_oculta3 = tf.nn.relu( tf.add( bias['decoder_oculta3'], tf.matmul(camada_oculta2, W['decoder_oculta3']) ) )
camada_saida = tf.nn.relu( tf.add( bias['decoder_saida'], tf.matmul(camada_oculta3, W['decoder_saida']) ) )

erro = tf.losses.mean_squared_error(xph, camada_saida)
otimizador = tf.train.AdamOptimizer(learning_rate= 0.001)
treinamento = otimizador.minimize(erro)
batch_size = 128

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  for epoch in range(50):
    num_bathes = mnist.train.num_examples // batch_size
    for i  in range(num_bathes):
      X_batch, _ = mnist.train.next_batch(batch_size)
      custo, _ = sess.run([erro, treinamento], feed_dict= {xph: X_batch})
    print(f'epoch {epoch} - erro {custo}')
  imagens_codificadas = sess.run(camada_oculta2, feed_dict={xph: X})
  imagens_decodificadas = sess.run(camada_saida, feed_dict={xph: X})

imagens_codificadas.shape

imagens_codificadas[0]

imagens_decodificadas.shape

imagens_decodificadas[0]

numero_imagens = 5
imagens_teste = np.random.randint(X.shape[0], size= numero_imagens)
imagens_teste

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.figure(figsize=(15,15))
for i, index_img in enumerate(imagens_teste):
  eixo = plt.subplot(5,5,i+1)
  plt.imshow(X[index_img].reshape(28,28))
  plt.xticks(())
  plt.yticks(())

  eixo = plt.subplot(5,5,i+1+numero_imagens)
  plt.imshow(imagens_codificadas[index_img].reshape(8,8))
  plt.xticks(())
  plt.yticks(())

  eixo = plt.subplot(5,5,i+1+numero_imagens * 2)
  plt.imshow(imagens_decodificadas[index_img].reshape(28,28))
  plt.xticks(())
  plt.yticks(())