# -*- coding: utf-8 -*-
"""tensorflow1_15___redes_neurais_recorrentes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kPR32ugNAK1W-vBAlV33zm1r-dBiZHgW
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error

"""> # **Aula 022** - Série temporal - **Tensorflow: Redes Neurais Recorrentes**
>> Exemplo: Preços da bolsa de valores
"""

base = pd.read_csv('petr4.csv')
base.head()

base.shape

base = base.dropna()
base.shape

base = base.iloc[:, 1].values
base

plt.plot(base)

periodos = 30
previsao_futura = 1

X = base[0:(len(base)- len(base) % periodos)]
X

X_batches = X.reshape(-1, periodos, 1)
X_batches

y = base[1:((len(base) - len(base) % periodos) + previsao_futura)]
y

y_batches = y.reshape(-1, periodos, 1)
y_batches

X_teste = base[-(periodos + previsao_futura):]
X_teste = X_teste[:periodos]
X_teste

X_teste = X_teste.reshape(-1, periodos, 1)
print(X_teste.shape)
X_teste

y_teste = base[-(periodos):]
y_teste

y_teste = y_teste.reshape(-1, periodos, 1)
print(y_teste.shape)
y_teste

tf.reset_default_graph()

entradas = 1
neuronios_camada_oculta = 100
neuronios_saida = 1

xph = tf.placeholder(tf.float32, [None, periodos, entradas])
yph = tf.placeholder(tf.float32, [None, periodos, neuronios_saida])

def create_cell():
  return tf.contrib.rnn.LSTMCell(num_units= neuronios_camada_oculta, activation= tf.nn.relu)

def create_multiple_cells(num_cells):
  celulas = tf.nn.rnn_cell.MultiRNNCell([create_cell() for i in range(num_cells)])
  #return tf.contrib.rnn.DropoutWrapper(celulas, output_keep_prob= 0.1)
  return celulas

#celula = tf.contrib.rnn.BasicRNNCell(num_units= neuronios_camada_oculta, activation= tf.nn.relu)
#celula = tf.contrib.rnn.LSTMCell(num_units= neuronios_camada_oculta, activation= tf.nn.relu)

celula = create_multiple_cells(4)
# mapeamento para camada de saida
celula = tf.contrib.rnn.OutputProjectionWrapper(celula, output_size= 1)

saida_rnn, _ = tf.nn.dynamic_rnn(celula, xph, dtype=tf.float32)
erro = tf.losses.mean_squared_error(labels= yph, predictions= saida_rnn)

otimizador = tf.train.AdamOptimizer(learning_rate= 0.001)
treinamento = otimizador.minimize(erro)

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())

  for epoch in range(1000):
    _, custo = sess.run([treinamento, erro], feed_dict={xph:X_batches, yph:y_batches})
    if(epoch % 100 == 0):
      print(f'epoch {epoch} - erro {custo}')
  
  previsoes = sess.run(saida_rnn, feed_dict={xph: X_teste})

y_teste2 = np.ravel(y_teste)
previsoes2 = np.ravel(previsoes)

mae = mean_absolute_error(y_teste2, previsoes2)
mae

plt.plot(y_teste2, color='b', label= 'Valor real')
plt.plot(y_teste2, 'o', color='b', markersize=4)
plt.plot(previsoes2, color='r', label= 'Valor previsto')
plt.plot(previsoes2, 'o', color='r', markersize=4)
plt.legend()