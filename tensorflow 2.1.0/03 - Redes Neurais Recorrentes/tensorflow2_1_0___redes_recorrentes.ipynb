{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow2.1.0___redes_recorrentes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNxjg1FIiX5_",
        "colab_type": "text"
      },
      "source": [
        "> # **TensorFlow  2.1.0** - ***03*** - *Redes Neurais Recorrentes*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcazZwQ4iOvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adc01ddd-1e34-4071-d891-935064905df6"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "\n",
        "import tensorflow as tf\n",
        "import random, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import imdb # https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F44Yrm0BjCvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a357a00d-53a1-449d-b35f-d2ca891cbe0d"
      },
      "source": [
        "# Pre-processamento dos dados\n",
        "number_of_words = 20000 # Para todos textos da base de dados\n",
        "max_len = 100 # Para cada texto da base\n",
        "\n",
        "(x_training, y_training), (x_test, y_test) = imdb.load_data(num_words= number_of_words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKFAosFnjMh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "b0eaa583-2447-486f-a9f5-a2992b3c6399"
      },
      "source": [
        "print(' - Shapes:')\n",
        "print(f'x_training.shape -> {x_training.shape}')\n",
        "print(f'x_test.shape -> {x_test.shape}')\n",
        "print(f'y_training.shape -> {y_training.shape}')\n",
        "print(f'y_test.shape -> {y_test.shape}', end='\\n'*2)\n",
        "print(f' - Dado:\\nx_training[0] ->\\n{x_training[0]}', end='\\n'*2)\n",
        "print(f' - Dado:\\nnp.unique(y_training) -> {np.unique(y_training)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Shapes:\n",
            "x_training.shape -> (25000,)\n",
            "x_test.shape -> (25000,)\n",
            "y_training.shape -> (25000,)\n",
            "y_test.shape -> (25000,)\n",
            "\n",
            " - Dado:\n",
            "x_training[0] ->\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "\n",
            " - Dado:\n",
            "np.unique(y_training) -> [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmHkQ7tjS_5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9cd28b19-22ef-4905-a108-91e7a010a5e7"
      },
      "source": [
        "# Visualizando tamanho dos dados\n",
        "print(' - Lens:')\n",
        "print(f'len(x_training[0]) -> {len(x_training[0])}')\n",
        "print(f'len(x_training[1]) -> {len(x_training[1])}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Lens:\n",
            "len(x_training[0]) -> 218\n",
            "len(x_training[1]) -> 189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39YDRjbFjU0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "4276ba7b-9ad5-4afc-a0fe-e47b4f58840f"
      },
      "source": [
        "# Normalizando dados\n",
        "print(' - Normalizando dados:')\n",
        "x_training = tf.keras.preprocessing.sequence.pad_sequences(x_training, maxlen= max_len)\n",
        "x_test     = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen= max_len)\n",
        "print(f'len(x_training[0]) -> {len(x_training[0])}')\n",
        "print(f'len(x_training[1]) -> {len(x_training[1])}')\n",
        "print(f'x_training.shape -> {x_training.shape}')\n",
        "print(f'x_test.shape -> {x_test.shape}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Normalizando dados:\n",
            "len(x_training[0]) -> 100\n",
            "len(x_training[1]) -> 100\n",
            "x_training.shape -> (25000, 100)\n",
            "x_test.shape -> (25000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1pvLmfjZQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "953df93e-6797-42a4-b163-4967506d8212"
      },
      "source": [
        "# Construindo a rede neural\n",
        "model = tf.keras.Sequential()\n",
        "# Adicionando a camada de Embedding\n",
        "model.add(tf.keras.layers.Embedding(input_dim= number_of_words, output_dim= 128, input_shape= (x_training.shape[1], )))\n",
        "# Adicionando a camadas de LSTM\n",
        "model.add(tf.keras.layers.LSTM(units= 128, activation= 'tanh'))\n",
        "# Adicionando camada de saida\n",
        "model.add(tf.keras.layers.Dense(units= 1, activation= 'sigmoid'))\n",
        "\n",
        "# Compilando o modelo\n",
        "model.compile(optimizer= 'rmsprop',loss= 'binary_crossentropy',metrics= ['accuracy'])\n",
        "print(' - Resumo do modelo')\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Resumo do modelo\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 128)          2560000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzGT5MLkjhDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "6abc0016-aaa3-4d2d-c5c4-1bfac991de00"
      },
      "source": [
        "# Treinamento do modelo\n",
        "print(' - Treinamento do modelo')\n",
        "model.fit(x_training, y_training, epochs= 5, batch_size= 128)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Treinamento do modelo\n",
            "Train on 25000 samples\n",
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 6s 231us/sample - loss: 0.1342 - accuracy: 0.9516\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 6s 224us/sample - loss: 0.1060 - accuracy: 0.9634\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 6s 223us/sample - loss: 0.0853 - accuracy: 0.9700\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 6s 225us/sample - loss: 0.0650 - accuracy: 0.9779\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 5s 219us/sample - loss: 0.0491 - accuracy: 0.9834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f119e350cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se3H13ZajoYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "871d44a6-8aad-4d10-de2c-2ed01a589d67"
      },
      "source": [
        "# Avaliando o modelo\n",
        "print(' - Avaliando o modelo')\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Taxa de acerto -> {test_accuracy}') \n",
        "print(f'Taxa de erro -> {test_loss}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Avaliando o modelo\n",
            "25000/25000 [==============================] - 4s 146us/sample - loss: 0.6266 - accuracy: 0.8208\n",
            "Taxa de acerto -> 0.8208000063896179\n",
            "Taxa de erro -> 0.6265767907071114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgQ1koM-jyOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f46daa41-f4cf-494c-a4d4-f76b13ca72d5"
      },
      "source": [
        "# Salvando o modelo\n",
        "print(' - Salvando o modelo ...')\n",
        "model_json = model.to_json()\n",
        "with open('imdb_model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "    print('Modelo salvo!')\n",
        "\n",
        "# Salvando os pesos\n",
        "print(' - Salvando os pesos do modelo ...')\n",
        "model.save_weights('imdb_model.h5')\n",
        "print('Pesos do modelos salvos!')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Salvando o modelo ...\n",
            "Modelo salvo!\n",
            " - Salvando os pesos do modelo ...\n",
            "Pesos do modelos salvos!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}