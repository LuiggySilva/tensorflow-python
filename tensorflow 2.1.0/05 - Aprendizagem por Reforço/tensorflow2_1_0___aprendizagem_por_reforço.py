# -*- coding: utf-8 -*-
"""tensorflow2.1.0___aprendizagem_por_reforço.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1chcKc3IehQ-qt6Cugrjcp66NJ3dGy4jF

> # **TensorFlow  2.1.0** - ***05*** - *Aprendizagem por Reforço*
"""

!pip install pandas-datareader

!pip install tensorflow-gpu==2.0.0.alpha0

import math
import random
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas_datareader as data_reader

from tqdm import tqdm_notebook, tqdm
from collections import deque
tf.__version__

# Classe IA Trader para negociação no mercado de ações

class AI_Trader():
  def __init__(self, state_size, action_space= 3, model_name= 'AITrader'):
    # Quantos estados existem no ambiente (Entrada)
    self.state_size = state_size 
    # Quantidade de ações possiveis
    self.action_space = action_space
    # Experiência de Replay
    self.memory = deque(maxlen= 2000)
    self.model_name = model_name

    # Fator de desconto da equação de Bellman
    self.gamma = 0.95
    # Indica se as ações sao tomadas randomicamente ou pela rede neural (inicialmete as ações são aleatorias)
    self.epsilon = 1.0
    self.epsilon_final = 0.01
    self.epsilon_decay = 0.995
    self.model = self.mode_builder()
  
  def mode_builder(self):
    # Rede Neural
    model = tf.keras.models.Sequential()
    # Camada de entrada e camanda oculta 1
    model.add(tf.keras.layers.Dense(units= 32,  activation= 'relu', input_dim= self.state_size))
    model.add(tf.keras.layers.Dense(units= 64,  activation= 'relu')) # Camada oculta 2
    model.add(tf.keras.layers.Dense(units= 128, activation= 'relu')) # Camada oculta 3
    # Camada de saida
    model.add(tf.keras.layers.Dense(units= self.action_space, activation= 'linear'))

    model.compile(loss= 'mse', optimizer= tf.keras.optimizers.Adam(lr = 0.001))
    return model
  
  def trade(self, state):
    if(random.random() <= self.epsilon):
      return random.randrange(self.action_space)
    
    actions = self.model.predict(state)
    return np.argmax(actions[0])
  
  def batch_train(self, batch_size):
    batch = []
    for i in range(len(self.memory) - batch_size + 1, len(self.memory)): # Os ultimos registros da memória
      batch.append(self.memory[i])
    
    for state, action, reward, next_state, done in batch:
      if not done:
        # Equação de Bellman
        reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])

      target = self.model.predict(state)
      target[0][action] = reward

      self.model.fit(state, target, epochs= 1, verbose= 0) # verbose = 0 -> sem log de treinamento
    
    if(self.epsilon > self.epsilon_final):
      self.epsilon *= self.epsilon_decay

# função sigmoid
def sigmoid(x):
  return 1 / (1 + math.exp(-x))

# função para formatação dos preços
def stocks_price_format(n):
  if (n < 0):
    return "- $ {0:2f}".format(abs(n))
  else:
    return "$ {0:2f}".format(abs(n))

print(sigmoid(0.5))
print(sigmoid(-0.5))
print(stocks_price_format(100))
print(stocks_price_format(-100))

# pré-processamento da base de dados
dataset = data_reader.DataReader('AAPL', data_source= 'yahoo')
print(str(dataset.index[0]).split()[0])
print(dataset.shape)
dataset.head()

dataset['Close']

def dataset_loader(stock_name):
  dataset = data_reader.DataReader(stock_name, data_source= 'yahoo')
  start_date = str(dataset.index[0]).split()[0]
  end_date = str(dataset.index[-1]).split()[0]
  close = dataset['Close']
  return close

def state_creator(data, timestep, window_size):
  starting_id = timestep - window_size + 1
  
  if starting_id >= 0:
    windowed_data = data[starting_id:timestep + 1]
  else:
    windowed_data = - starting_id * [data[0]] + list(data[0:timestep + 1])
    
  state = []
  for i in range(window_size - 1):
    state.append(sigmoid(windowed_data[i + 1] - windowed_data[i]))
    
  return np.array([state]), windowed_data

# carregando base de dados
stock_name = 'AAPL'
data = dataset_loader(stock_name)
data.head()

# hyper parametros
window_size = 10
episodes = 1
batch_size = 32
data_samples = len(data)-1
# definição do modelo
trader = AI_Trader(window_size)
trader.model.summary()

# treinamento
for episode in range(1, episodes + 1):
  print("Episode: {}/{}".format(episode, episodes))
  state = state_creator(data, 0, window_size + 1)
  total_profit = 0
  trader.inventory = []
  for t in tqdm(range(data_samples)):
    action = trader.trade(state)
    next_state = state_creator(data, t + 1, window_size + 1)
    reward = 0
    
    if action == 1: # Comprando uma ação
      trader.inventory.append(data[t])
      print("AI Trader bought: ", stocks_price_format(data[t]))
    elif action == 2 and len(trader.inventory) > 0: # Vendendo uma ação  
      buy_price = trader.inventory.pop(0)
      
      reward = max(data[t] - buy_price, 0)
      total_profit += data[t] - buy_price
      print("AI Trader sold: ", stocks_price_format(data[t]), " Profit: " + stocks_price_format(data[t] - buy_price))
      
    if t == data_samples - 1:
      done = True
    else:
      done = False
      
    trader.memory.append((state, action, reward, next_state, done))
    state = next_state
    
    if done:
      print("#" * 20)
      print(" - Total profit: {}".format(total_profit))
      print("#" * 20)
      
    if len(trader.memory) > batch_size:
      trader.batch_train(batch_size)
     
  if episode % 10 == 0:
    trader.model.save("ai_trader_{}.h5".format(episode))