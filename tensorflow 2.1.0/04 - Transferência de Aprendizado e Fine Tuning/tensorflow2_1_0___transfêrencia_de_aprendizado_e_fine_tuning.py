# -*- coding: utf-8 -*-
"""tensorflow2.1.0___transfêrencia_de_aprendizado_e_fine_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zp7pRQBUzeI0Jo0UihRtV1rnI4rK8JAw

> # **TensorFlow  2.1.0** - ***04*** - *Transferência de Aprendizado e Fine Tuning*
"""

# Commented out IPython magic to ensure Python compatibility.
try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass

import os
import zipfile
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from tqdm import tqdm_notebook
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Download do dataset
'''
!wget --no-check-certificate \
    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \
    -O ./cats_and_dogs_filtered.zip
'''

# Descompactando e configurando paths do dataset
dataset_path = "./cats_and_dogs_filtered.zip"
zip_object = zipfile.ZipFile(file=dataset_path, mode="r")
zip_object.extractall("./")
zip_object.close()

dataset_path = './cats_and_dogs_filtered'
training_dir = os.path.join(dataset_path, 'train')
validation_dir = os.path.join(dataset_path, 'validation')

# Carregando modeloo pré-treinando (MobileNetV2)
img_shape = (128, 128, 3) # MobileNetV2 suporta: (96, 96), (128, 128), (160, 160), (192, 129), (224, 224)
# include_top = False -> cabeçalho(saida) personalizado | include_top = True -> usar toda estrutura 
# (deve ter compatibilidade na saida, se no MobileNetV2 tiver 1000 saidas, nossa rede tambem deve ter 1000 saidas)
base_model = tf.keras.applications.MobileNetV2(input_shape= img_shape, include_top= False, weights= 'imagenet')
print(' - Resumo do modelo')
base_model.summary()

# Congelando modelo base
base_model.trainable = False # Não vai alterar os pesos

# Definindo cabeçalho personalizado da rede neural (camada de saida)
print(base_model.output)

# Reduz a dimensionalidade dos dados
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
print(global_average_layer)

# camada de saida
prediction_layer = tf.keras.layers.Dense(units= 1, activation= 'sigmoid')(global_average_layer)

# Definindo modelo
model = tf.keras.models.Model(inputs= base_model.input, outputs= prediction_layer)
print(' - Resumo do modelo completo')
model.summary()

# Compilando modelo
model.compile(optimizer= tf.keras.optimizers.RMSprop(lr= 0.0001), 
              loss= 'binary_crossentropy', metrics= ['accuracy'])

# Data Generetos
data_gen_training = ImageDataGenerator(rescale= 1/255.)
data_gen_validation = ImageDataGenerator(rescale= 1/255.)

training_generator = data_gen_training.flow_from_directory(training_dir, target_size= (128, 128),
                                                           batch_size= 128, class_mode= 'binary')

validation_generator = data_gen_validation.flow_from_directory(validation_dir, target_size= (128, 128),
                                                           batch_size= 128, class_mode= 'binary')

# Treinamento do modelo
print('\n'*2 + ' - Treinamento do modelo')
model.fit_generator(training_generator, epochs= 10, validation_data= validation_generator)

# Avaliação do modelo
print('\n'*2 + ' - Avaliando o modelo')
valid_loss, valid_accuracy = model.evaluate_generator(validation_generator)
print(f'valididation_accuracy - {valid_accuracy}')
print(f'valididation_loss - {valid_loss}')

# Fine Tuning
base_model.trainable = True
print(len(base_model.layers))

fine_tunning_at = 100
for layer in base_model.layers[:fine_tunning_at]:
  layer.trainable = False

# Compilando o modelo para Fine Tuning
model.compile(optimizer= tf.keras.optimizers.RMSprop(lr= 0.0001), loss= 'binary_crossentropy',
              metrics= ['accuracy'])

# Aplicando o Fine Tuning
print('\n'*2 + ' - Treinamento do modelo com Fine Tuning')
model.fit_generator(training_generator, epochs= 10, validation_data= validation_generator)

# Avaliação do modelo com Fine Tuning
print('\n'*2 + ' - Avaliando o modelo com Fine Tuning')
valid_loss, valid_accuracy = model.evaluate_generator(validation_generator)
print(f'valididation_accuracy - {valid_accuracy}')
print(f'valididation_loss - {valid_loss}')